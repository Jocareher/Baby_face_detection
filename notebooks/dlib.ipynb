{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def resize_image(image: np.ndarray, max_size: int = 800) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resize the image while maintaining aspect ratio, so that the maximum dimension is not greater than max_size.\n",
    "    \n",
    "    Args:\n",
    "        - image (numpy.ndarray): Input image.\n",
    "        - max_size (int): Maximum size for the larger dimension after resizing. Default is 800.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resized image.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    if max(height, width) > max_size:\n",
    "        if height > width:\n",
    "            new_height = max_size\n",
    "            new_width = int(width * (max_size / height))\n",
    "        else:\n",
    "            new_width = max_size\n",
    "            new_height = int(height * (max_size / width))\n",
    "        image = cv2.resize(image, (new_width, new_height))\n",
    "    return image\n",
    "\n",
    "def dlib_cnn_detect_faces(source_folder: str, destination_folder: str, model_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Detect faces using DLIB's CNN face detector and save bounding boxes to text files.\n",
    "    \n",
    "    Args:\n",
    "        - source_folder (str): Path to the source directory containing image files.\n",
    "        - destination_folder (str): Path to the destination directory where processed images and labels will be saved.\n",
    "        - model_path (str): Path to the pre-trained model for face detection.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "    \n",
    "    # Ensure the destination folders exist\n",
    "    images_folder = os.path.join(destination_folder, \"images\")\n",
    "    labels_folder = os.path.join(destination_folder, \"labels\")\n",
    "    if not os.path.exists(images_folder):\n",
    "        os.makedirs(images_folder)\n",
    "    if not os.path.exists(labels_folder):\n",
    "        os.makedirs(labels_folder)\n",
    "    \n",
    "    # Load the CNN face detector\n",
    "    cnn_detector = dlib.cnn_face_detection_model_v1(model_path)\n",
    "    \n",
    "    # Iterate through all files in the source directory\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')) and not filename.startswith('.'):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            # Load the image\n",
    "            # Skip files that are not regular image files\n",
    "            try:\n",
    "                image = cv2.imread(file_path)\n",
    "            except:\n",
    "                print(f\"Skipping file: {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Resize the image while maintaining aspect ratio\n",
    "            image = resize_image(image)\n",
    "            \n",
    "            # Record start time\n",
    "            start = time.time()\n",
    "            \n",
    "            # Perform face detection\n",
    "            detections = cnn_detector(image, 2)\n",
    "            \n",
    "            for i, face in enumerate(detections):\n",
    "                l, t, r, b = face.rect.left(), face.rect.top(), face.rect.right(), face.rect.bottom()\n",
    "                \n",
    "                # Ensure x1, y1, x2, y2 format\n",
    "                x1, y1, x2, y2 = min(l, r), min(t, b), max(l, r), max(t, b)\n",
    "                \n",
    "                # Draw bounding boxes for each detected face\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                \n",
    "                # Save bounding box coordinates to a text file\n",
    "                label_file_path = os.path.join(labels_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "                with open(label_file_path, 'w') as label_file:\n",
    "                    label_file.write(f\"{x1} {y1} {x2} {y2}\\n\")\n",
    "            \n",
    "            # Record end time\n",
    "            end = time.time()\n",
    "            \n",
    "            print(f\"Processed {filename} in {end - start:.2f} seconds.\")\n",
    "            \n",
    "            # Save the processed image to the images folder\n",
    "            output_image_path = os.path.join(images_folder, filename)\n",
    "            cv2.imwrite(output_image_path, image)\n",
    "    print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlib_cnn_detect_faces_score(source_folder: str, destination_folder: str, model_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Detect faces using DLIB's CNN face detector and save bounding boxes to text files with confidence scores.\n",
    "    \n",
    "    Args:\n",
    "        - source_folder (str): Path to the source directory containing image files.\n",
    "        - destination_folder (str): Path to the destination directory where processed images and labels will be saved.\n",
    "        - model_path (str): Path to the pre-trained model for face detection.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "    \n",
    "    images_folder = os.path.join(destination_folder, \"images\")\n",
    "    labels_folder = os.path.join(destination_folder, \"labels\")\n",
    "    os.makedirs(images_folder, exist_ok=True)\n",
    "    os.makedirs(labels_folder, exist_ok=True)\n",
    "    \n",
    "    cnn_detector = dlib.cnn_face_detection_model_v1(model_path)\n",
    "    \n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')) and not filename.startswith('.'):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            try:\n",
    "                image = cv2.imread(file_path)\n",
    "            except:\n",
    "                print(f\"Skipping file: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            image = resize_image(image)\n",
    "            start = time.time()\n",
    "            detections = cnn_detector(image, 2)\n",
    "\n",
    "            label_file_path = os.path.join(labels_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "            with open(label_file_path, 'a') as label_file:  # Use 'a' to append if processing multiple faces\n",
    "                for i, face in enumerate(detections):\n",
    "                    l, t, r, b = face.rect.left(), face.rect.top(), face.rect.right(), face.rect.bottom()\n",
    "                    confidence = face.confidence\n",
    "                    \n",
    "                    x1, y1, x2, y2 = min(l, r), min(t, b), max(l, r), max(t, b)\n",
    "                    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    \n",
    "                    # Include confidence value with bounding box coordinates\n",
    "                    label_file.write(f\"{confidence} {x1} {y1} {x2} {y2}\\n\")\n",
    "\n",
    "            end = time.time()\n",
    "            print(f\"Processed {filename} in {end - start:.2f} seconds.\")\n",
    "            \n",
    "            output_image_path = os.path.join(images_folder, filename)\n",
    "            cv2.imwrite(output_image_path, image)\n",
    "    print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed flip_face_img_8644.jpg in 9.44 seconds.\n",
      "Processed flip_face_img_5017.jpg in 4.84 seconds.\n",
      "Processed flip_face_img_3314.jpg in 5.45 seconds.\n",
      "Processed flip_face_img_1073.jpg in 4.21 seconds.\n",
      "Processed face_bcn_507.jpg in 7.62 seconds.\n",
      "Processed face_bcn_249.jpg in 8.65 seconds.\n",
      "Processed face_img_5313.jpg in 3.62 seconds.\n",
      "Processed face_img_2118.jpg in 5.95 seconds.\n",
      "Processed face_img_1388.jpg in 3.80 seconds.\n",
      "Processed flip_face_img_3470.jpg in 4.52 seconds.\n",
      "Processed flip_face_img_7549.jpg in 6.13 seconds.\n",
      "Processed flip_face_bcn_540.jpg in 5.76 seconds.\n",
      "Processed flip_face_bcn_187.jpg in 6.59 seconds.\n",
      "Processed flip_face_img_6040.jpg in 4.22 seconds.\n",
      "Processed face_img_4431.jpg in 6.27 seconds.\n",
      "Processed face_img_7660.jpg in 3.16 seconds.\n",
      "Processed face_img_6998.jpg in 3.13 seconds.\n",
      "Processed face_img_2320.jpg in 6.66 seconds.\n",
      "Processed face_bcn_307.jpg in 5.11 seconds.\n",
      "Processed face_img_8369.jpg in 3.38 seconds.\n",
      "Processed face_img_6393.jpg in 5.80 seconds.\n",
      "Processed face_bcn_448.jpg in 7.79 seconds.\n",
      "Processed face_img_650.jpg in 5.99 seconds.\n",
      "Processed flip_face_img_6679.jpg in 3.16 seconds.\n",
      "Processed flip_face_bcn_11.jpg in 7.65 seconds.\n",
      "Processed face_img_6580.jpg in 6.50 seconds.\n",
      "Processed flip_face_img_8456.jpg in 3.63 seconds.\n",
      "Processed face_img_8591.jpg in 3.02 seconds.\n",
      "Processed flip_face_bcn_230.jpg in 9.39 seconds.\n",
      "Processed face_img_6040.jpg in 4.25 seconds.\n",
      "Processed flip_face_img_477.jpg in 4.91 seconds.\n",
      "Processed flip_face_img_4431.jpg in 7.36 seconds.\n",
      "Processed extra_faces_43_copy_1.jpg in 6.17 seconds.\n",
      "Processed flip_face_bcn_652.jpg in 7.71 seconds.\n",
      "Processed flip_face_bcn_268.jpg in 8.97 seconds.\n",
      "Processed flip_face_bcn_75.jpg in 5.84 seconds.\n",
      "Processed face_img_6679.jpg in 3.11 seconds.\n",
      "Processed flip_face_img_6580.jpg in 7.21 seconds.\n",
      "Processed flip_face_img_8591.jpg in 3.19 seconds.\n",
      "Processed face_img_8456.jpg in 3.57 seconds.\n",
      "Processed flip_face_img_931.jpg in 7.08 seconds.\n",
      "Processed flip_face_img_7660.jpg in 3.13 seconds.\n",
      "Processed face_bcn_175.jpg in 7.24 seconds.\n",
      "Processed face_img_4195_copy_1.jpg in 4.73 seconds.\n",
      "Processed flip_face_bcn_447.jpg in 8.20 seconds.\n",
      "Processed flip_face_bcn_321.jpg in 7.11 seconds.\n",
      "Processed flip_face_img_6998.jpg in 3.72 seconds.\n",
      "Processed flip_face_img_2320.jpg in 8.00 seconds.\n",
      "Processed flip_face_img_8369.jpg in 4.39 seconds.\n",
      "Processed flip_face_img_6393.jpg in 7.08 seconds.\n",
      "Processed face_img_3314.jpg in 4.67 seconds.\n",
      "Processed face_img_5017.jpg in 6.13 seconds.\n",
      "Processed face_img_1073.jpg in 3.94 seconds.\n",
      "Processed face_img_8644.jpg in 8.14 seconds.\n",
      "Processed face_bcn_399.jpg in 8.98 seconds.\n",
      "Processed face_img_3101.jpg in 3.37 seconds.\n",
      "Processed flip_face_bcn_73.jpg in 6.44 seconds.\n",
      "Processed face_img_7549.jpg in 6.71 seconds.\n",
      "Processed face_bcn_166.jpg in 7.02 seconds.\n",
      "Processed flip_face_bcn_131.jpg in 5.65 seconds.\n",
      "Processed flip_face_img_5313.jpg in 3.49 seconds.\n",
      "Processed flip_face_img_2118.jpg in 6.26 seconds.\n",
      "Processed flip_face_img_1388.jpg in 3.51 seconds.\n",
      "Processed face_img_3470.jpg in 4.47 seconds.\n",
      "Processed face_img_2788.jpg in 3.44 seconds.\n",
      "Processed face_img_6061.jpg in 3.66 seconds.\n",
      "Processed flip_face_img_3784.jpg in 5.47 seconds.\n",
      "Processed flip_face_img_1541.jpg in 3.54 seconds.\n",
      "Processed flip_face_bcn_249.jpg in 7.07 seconds.\n",
      "Processed flip_face_bcn_507.jpg in 5.25 seconds.\n",
      "Processed flip_face_img_6990.jpg in 5.22 seconds.\n",
      "Processed face_bcn_540.jpg in 7.27 seconds.\n",
      "Processed face_bcn_187.jpg in 8.48 seconds.\n",
      "Processed face_img_1319.jpg in 4.01 seconds.\n",
      "Processed face_img_2604.jpg in 3.49 seconds.\n",
      "Processed flip_face_img_5523.jpg in 7.52 seconds.\n",
      "Processed face_bcn_11.jpg in 6.92 seconds.\n",
      "Processed flip_face_img_5292.jpg in 0.17 seconds.\n",
      "Processed flip_face_img_8359.jpg in 3.99 seconds.\n",
      "Processed flip_face_img_3965.jpg in 3.52 seconds.\n",
      "Processed flip_face_img_6000.jpg in 0.13 seconds.\n",
      "Processed flip_face_bcn_307.jpg in 6.25 seconds.\n",
      "Processed flip_face_bcn_448.jpg in 7.47 seconds.\n",
      "Processed flip_face_img_6968.jpg in 3.40 seconds.\n",
      "Processed face_bcn_230.jpg in 8.43 seconds.\n",
      "Processed face_img_7435.jpg in 0.25 seconds.\n",
      "Processed face_img_3323.jpg in 3.33 seconds.\n",
      "Processed flip_face_img_650.jpg in 7.28 seconds.\n",
      "Processed flip_face_img_2604.jpg in 3.29 seconds.\n",
      "Processed flip_face_img_1319.jpg in 3.64 seconds.\n",
      "Processed face_img_477.jpg in 4.16 seconds.\n",
      "Processed face_img_5523.jpg in 7.23 seconds.\n",
      "Processed face_img_5292.jpg in 0.18 seconds.\n",
      "Processed face_bcn_75.jpg in 5.65 seconds.\n",
      "Processed face_img_8359.jpg in 4.16 seconds.\n",
      "Processed face_bcn_652.jpg in 8.17 seconds.\n",
      "Processed face_img_6968.jpg in 3.86 seconds.\n",
      "Processed face_bcn_268.jpg in 9.24 seconds.\n",
      "Processed face_img_931.jpg in 6.59 seconds.\n",
      "Processed flip_face_img_7435.jpg in 0.25 seconds.\n",
      "Processed flip_face_img_3323.jpg in 3.31 seconds.\n",
      "Processed face_bcn_447.jpg in 8.17 seconds.\n",
      "Processed face_bcn_321.jpg in 7.46 seconds.\n",
      "Processed face_img_3965.jpg in 3.62 seconds.\n",
      "Processed flip_face_bcn_175.jpg in 7.35 seconds.\n",
      "Processed face_img_6000.jpg in 0.13 seconds.\n",
      "Processed face_img_1541.jpg in 3.80 seconds.\n",
      "Processed face_img_3101_copy_1.jpg in 2.79 seconds.\n",
      "Processed face_bcn_73.jpg in 5.70 seconds.\n",
      "Processed flip_extra_faces_55.jpg in 4.45 seconds.\n",
      "Processed flip_face_bcn_399.jpg in 7.44 seconds.\n",
      "Processed flip_face_img_2788.jpg in 3.52 seconds.\n",
      "Processed flip_face_img_6061.jpg in 3.42 seconds.\n",
      "Processed face_img_3784.jpg in 4.56 seconds.\n",
      "Processed flip_face_bcn_166.jpg in 6.58 seconds.\n",
      "Processed extra_faces_55.jpg in 4.45 seconds.\n",
      "Processed face_bcn_131.jpg in 5.25 seconds.\n",
      "Processed face_img_6990.jpg in 5.37 seconds.\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "source_folder = \"/Users/jocareher/Downloads/benchmark_test_split_rotated/4/images\"\n",
    "destination_folder = \"/Users/jocareher/Downloads/dlib_predictions_new/4\"\n",
    "model_path = \"/Users/jocareher/Library/CloudStorage/OneDrive-Personal/Educación/eLearning/Face_Detection/Weights/mmod_human_face_detector.dat\"\n",
    "dlib_cnn_detect_faces(source_folder=source_folder,\n",
    "                      destination_folder=destination_folder,\n",
    "                      model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed flip_face_img_8491.jpg in 4.36 seconds.\n",
      "Processed flip_face_img_6866.jpg in 4.98 seconds.\n",
      "Processed face_img_1957.jpg in 3.38 seconds.\n",
      "Processed flip_face_img_4730.jpg in 2.96 seconds.\n",
      "Processed face_bcn_510.jpg in 6.31 seconds.\n",
      "Processed flip_face_img_5406.jpg in 3.18 seconds.\n",
      "Processed flip_face_img_4888.jpg in 4.61 seconds.\n",
      "Processed face_img_8191.jpg in 4.92 seconds.\n",
      "Processed face_img_4218.jpg in 6.08 seconds.\n",
      "Processed flip_face_img_8283.jpg in 3.55 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m destination_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/jocareher/Downloads/dlib_predictions_score/1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/jocareher/Library/CloudStorage/OneDrive-Personal/Educación/eLearning/Face_Detection/Weights/mmod_human_face_detector.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdlib_cnn_detect_faces_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdestination_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestination_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mdlib_cnn_detect_faces_score\u001b[0;34m(source_folder, destination_folder, model_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m image \u001b[38;5;241m=\u001b[39m resize_image(image)\n\u001b[1;32m     30\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 31\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m label_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(labels_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(label_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m label_file:  \u001b[38;5;66;03m# Use 'a' to append if processing multiple faces\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "source_folder =\"/Users/jocareher/Downloads/benchmark_test_split_rotated/1/images\"\n",
    "destination_folder = \"/Users/jocareher/Downloads/dlib_predictions_score/1\"\n",
    "model_path = \"/Users/jocareher/Library/CloudStorage/OneDrive-Personal/Educación/eLearning/Face_Detection/Weights/mmod_human_face_detector.dat\"\n",
    "dlib_cnn_detect_faces_score(source_folder=source_folder,\n",
    "                      destination_folder=destination_folder,\n",
    "                      model_path=model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
